{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP5C1BY3DcY156bjLg/kVxq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunshine66980/LLM/blob/main/RAG_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 安装依赖（重启运行时后运行此单元格）\n",
        "!pip install -U transformers FlagEmbedding faiss-cpu sentence-transformers langchain accelerate requests python-dotenv\n"
      ],
      "metadata": {
        "id": "MQttgvQPcvZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 临时设置\n",
        "%env SILICONFLOW_API_KEY=\n",
        "!echo $SILICONFLOW_API_KEY           # 验证\n",
        "\n",
        "import os\n",
        "print(\"当前API_KEY:\", os.getenv(\"SILICONFLOW_API_KEY\"))  # 确认密钥加载"
      ],
      "metadata": {
        "id": "3V874-nSiFm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. 导入库\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "import requests\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import numpy as np\n",
        "\n",
        "# 3. 初始化BGE-M3嵌入模型（使用T4 GPU加速）\n",
        "model_name = \"BAAI/bge-m3\"\n",
        "model_kwargs = {\"device\": \"cuda\"}  # 使用GPU加速\n",
        "encode_kwargs = {\n",
        "    \"normalize_embeddings\": True,  # 归一化向量提升精度\n",
        "    \"query_instruction\": \"\"        # BGE-M3需要空指令\n",
        "}\n",
        "\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "print(\"✅ BGE-M3嵌入模型加载完成，使用T4 GPU加速\")\n",
        "\n",
        "# 4. 配置硅基流动API（在Colab左侧钥匙图标中添加环境变量SILICONFLOW_API_KEY）\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"SILICONFLOW_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"请添加SILICONFLOW_API_KEY环境变量\")\n",
        "\n",
        "# 硅基流动API调用函数\n",
        "def call_siliconflow(prompt: str, model=\"deepseek-ai/DeepSeek-R1\"):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"max_tokens\": 500\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"https://api.siliconflow.cn/v1/chat/completions\",\n",
        "            headers=headers,\n",
        "            json=data,\n",
        "            timeout=30\n",
        "        )\n",
        "        response.raise_for_status()  # 检查HTTP状态码\n",
        "        response_data = response.json()\n",
        "        if \"choices\" in response_data:\n",
        "            return response_data[\"choices\"][0][\"message\"][\"content\"]\n",
        "        else:\n",
        "            raise ValueError(f\"无效响应结构: {response_data}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"API请求失败: {e}\\n响应文本: {response.text if 'response' in locals() else '无响应'}\")\n",
        "        return \"请求失败，请检查网络或API配置\"\n",
        "    except ValueError as e:\n",
        "        print(f\"API响应解析失败: {e}\\n原始响应: {response.text}\")\n",
        "        return \"解析响应时发生错误\"\n",
        "\n",
        "# 5. 准备知识库文档\n",
        "documents = [\n",
        "    \"BGE-M3支持多向量检索和稀疏检索，适用于长文档处理（最大支持8192 tokens）\",\n",
        "    \"在知识图谱构建中，BGE-M3能有效合并相似实体（如‘孙悟空’和‘悟空’）\",\n",
        "    \"硅基流动的DeepSeek-R1模型支持128K上下文长度，适合处理超长文本\",\n",
        "    \"RAG通过检索外部知识库增强生成结果，减少模型幻觉\",\n",
        "    \"教学时建议分块大小为500-1000字符，重叠50字符以保持语义连贯\"\n",
        "]\n",
        "\n",
        "# 6. 文本分块处理\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,      # 分块大小\n",
        "    chunk_overlap=50,    # 重叠字符数\n",
        "    length_function=len  # 长度计算函数\n",
        ")\n",
        "texts = text_splitter.create_documents(documents)\n",
        "print(f\"📚 知识库分割为 {len(texts)} 个文本块\")\n",
        "\n",
        "# 7. 创建向量数据库（FAISS）\n",
        "vector_db = FAISS.from_documents(texts, embeddings)\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})  # 返回Top3结果\n",
        "print(\"🔍 FAISS向量数据库构建完成\")\n",
        "\n",
        "# 8. 检索增强生成函数\n",
        "def rag_query(question: str):\n",
        "    # 检索相关文档\n",
        "    retrieved_docs = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # 构造增强提示\n",
        "    prompt = f\"\"\"基于以下上下文回答问题：\n",
        "{context}\n",
        "问题：{question}\n",
        "要求：回答需简洁准确，并标注引用来源编号[1-3]\"\"\"\n",
        "\n",
        "    # 调用大模型生成答案\n",
        "    answer = call_siliconflow(prompt)\n",
        "\n",
        "    # 打印结果\n",
        "    print(\"=\"*50)\n",
        "    print(f\"❓ 问题: {question}\")\n",
        "    print(\"-\"*50)\n",
        "    print(f\"📄 检索到的上下文:\\n{context}\")\n",
        "    print(\"-\"*50)\n",
        "    print(f\"💡 生成的答案:\\n{answer}\")\n",
        "    print(\"=\"*50)\n",
        "    return answer\n",
        "\n",
        "# 9. 测试RAG流程\n",
        "rag_query(\"BGE-M3如何处理长文档？\")\n",
        "rag_query(\"RAG在教学中有哪些优势？\")"
      ],
      "metadata": {
        "id": "JFnw5ePPcSZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}