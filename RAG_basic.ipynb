{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP5C1BY3DcY156bjLg/kVxq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunshine66980/LLM/blob/main/RAG_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. å®‰è£…ä¾èµ–ï¼ˆé‡å¯è¿è¡Œæ—¶åè¿è¡Œæ­¤å•å…ƒæ ¼ï¼‰\n",
        "!pip install -U transformers FlagEmbedding faiss-cpu sentence-transformers langchain accelerate requests python-dotenv\n"
      ],
      "metadata": {
        "id": "MQttgvQPcvZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # ä¸´æ—¶è®¾ç½®\n",
        "%env SILICONFLOW_API_KEY=\n",
        "!echo $SILICONFLOW_API_KEY           # éªŒè¯\n",
        "\n",
        "import os\n",
        "print(\"å½“å‰API_KEY:\", os.getenv(\"SILICONFLOW_API_KEY\"))  # ç¡®è®¤å¯†é’¥åŠ è½½"
      ],
      "metadata": {
        "id": "3V874-nSiFm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. å¯¼å…¥åº“\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "import requests\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import numpy as np\n",
        "\n",
        "# 3. åˆå§‹åŒ–BGE-M3åµŒå…¥æ¨¡å‹ï¼ˆä½¿ç”¨T4 GPUåŠ é€Ÿï¼‰\n",
        "model_name = \"BAAI/bge-m3\"\n",
        "model_kwargs = {\"device\": \"cuda\"}  # ä½¿ç”¨GPUåŠ é€Ÿ\n",
        "encode_kwargs = {\n",
        "    \"normalize_embeddings\": True,  # å½’ä¸€åŒ–å‘é‡æå‡ç²¾åº¦\n",
        "    \"query_instruction\": \"\"        # BGE-M3éœ€è¦ç©ºæŒ‡ä»¤\n",
        "}\n",
        "\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "print(\"âœ… BGE-M3åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨T4 GPUåŠ é€Ÿ\")\n",
        "\n",
        "# 4. é…ç½®ç¡…åŸºæµåŠ¨APIï¼ˆåœ¨Colabå·¦ä¾§é’¥åŒ™å›¾æ ‡ä¸­æ·»åŠ ç¯å¢ƒå˜é‡SILICONFLOW_API_KEYï¼‰\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"SILICONFLOW_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"è¯·æ·»åŠ SILICONFLOW_API_KEYç¯å¢ƒå˜é‡\")\n",
        "\n",
        "# ç¡…åŸºæµåŠ¨APIè°ƒç”¨å‡½æ•°\n",
        "def call_siliconflow(prompt: str, model=\"deepseek-ai/DeepSeek-R1\"):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"max_tokens\": 500\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"https://api.siliconflow.cn/v1/chat/completions\",\n",
        "            headers=headers,\n",
        "            json=data,\n",
        "            timeout=30\n",
        "        )\n",
        "        response.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç \n",
        "        response_data = response.json()\n",
        "        if \"choices\" in response_data:\n",
        "            return response_data[\"choices\"][0][\"message\"][\"content\"]\n",
        "        else:\n",
        "            raise ValueError(f\"æ— æ•ˆå“åº”ç»“æ„: {response_data}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"APIè¯·æ±‚å¤±è´¥: {e}\\nå“åº”æ–‡æœ¬: {response.text if 'response' in locals() else 'æ— å“åº”'}\")\n",
        "        return \"è¯·æ±‚å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–APIé…ç½®\"\n",
        "    except ValueError as e:\n",
        "        print(f\"APIå“åº”è§£æå¤±è´¥: {e}\\nåŸå§‹å“åº”: {response.text}\")\n",
        "        return \"è§£æå“åº”æ—¶å‘ç”Ÿé”™è¯¯\"\n",
        "\n",
        "# 5. å‡†å¤‡çŸ¥è¯†åº“æ–‡æ¡£\n",
        "documents = [\n",
        "    \"BGE-M3æ”¯æŒå¤šå‘é‡æ£€ç´¢å’Œç¨€ç–æ£€ç´¢ï¼Œé€‚ç”¨äºé•¿æ–‡æ¡£å¤„ç†ï¼ˆæœ€å¤§æ”¯æŒ8192 tokensï¼‰\",\n",
        "    \"åœ¨çŸ¥è¯†å›¾è°±æ„å»ºä¸­ï¼ŒBGE-M3èƒ½æœ‰æ•ˆåˆå¹¶ç›¸ä¼¼å®ä½“ï¼ˆå¦‚â€˜å­™æ‚Ÿç©ºâ€™å’Œâ€˜æ‚Ÿç©ºâ€™ï¼‰\",\n",
        "    \"ç¡…åŸºæµåŠ¨çš„DeepSeek-R1æ¨¡å‹æ”¯æŒ128Kä¸Šä¸‹æ–‡é•¿åº¦ï¼Œé€‚åˆå¤„ç†è¶…é•¿æ–‡æœ¬\",\n",
        "    \"RAGé€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†åº“å¢å¼ºç”Ÿæˆç»“æœï¼Œå‡å°‘æ¨¡å‹å¹»è§‰\",\n",
        "    \"æ•™å­¦æ—¶å»ºè®®åˆ†å—å¤§å°ä¸º500-1000å­—ç¬¦ï¼Œé‡å 50å­—ç¬¦ä»¥ä¿æŒè¯­ä¹‰è¿è´¯\"\n",
        "]\n",
        "\n",
        "# 6. æ–‡æœ¬åˆ†å—å¤„ç†\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,      # åˆ†å—å¤§å°\n",
        "    chunk_overlap=50,    # é‡å å­—ç¬¦æ•°\n",
        "    length_function=len  # é•¿åº¦è®¡ç®—å‡½æ•°\n",
        ")\n",
        "texts = text_splitter.create_documents(documents)\n",
        "print(f\"ğŸ“š çŸ¥è¯†åº“åˆ†å‰²ä¸º {len(texts)} ä¸ªæ–‡æœ¬å—\")\n",
        "\n",
        "# 7. åˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆFAISSï¼‰\n",
        "vector_db = FAISS.from_documents(texts, embeddings)\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})  # è¿”å›Top3ç»“æœ\n",
        "print(\"ğŸ” FAISSå‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ\")\n",
        "\n",
        "# 8. æ£€ç´¢å¢å¼ºç”Ÿæˆå‡½æ•°\n",
        "def rag_query(question: str):\n",
        "    # æ£€ç´¢ç›¸å…³æ–‡æ¡£\n",
        "    retrieved_docs = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # æ„é€ å¢å¼ºæç¤º\n",
        "    prompt = f\"\"\"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n",
        "{context}\n",
        "é—®é¢˜ï¼š{question}\n",
        "è¦æ±‚ï¼šå›ç­”éœ€ç®€æ´å‡†ç¡®ï¼Œå¹¶æ ‡æ³¨å¼•ç”¨æ¥æºç¼–å·[1-3]\"\"\"\n",
        "\n",
        "    # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ\n",
        "    answer = call_siliconflow(prompt)\n",
        "\n",
        "    # æ‰“å°ç»“æœ\n",
        "    print(\"=\"*50)\n",
        "    print(f\"â“ é—®é¢˜: {question}\")\n",
        "    print(\"-\"*50)\n",
        "    print(f\"ğŸ“„ æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡:\\n{context}\")\n",
        "    print(\"-\"*50)\n",
        "    print(f\"ğŸ’¡ ç”Ÿæˆçš„ç­”æ¡ˆ:\\n{answer}\")\n",
        "    print(\"=\"*50)\n",
        "    return answer\n",
        "\n",
        "# 9. æµ‹è¯•RAGæµç¨‹\n",
        "rag_query(\"BGE-M3å¦‚ä½•å¤„ç†é•¿æ–‡æ¡£ï¼Ÿ\")\n",
        "rag_query(\"RAGåœ¨æ•™å­¦ä¸­æœ‰å“ªäº›ä¼˜åŠ¿ï¼Ÿ\")"
      ],
      "metadata": {
        "id": "JFnw5ePPcSZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}